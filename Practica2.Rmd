---
title: "Parte 2"
author: "Pablo"
date: "2024-01-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
summary(cars)
```

**Pregunta 1:**

**\*\*Queremos programar un programa de tipo web scrapping con el que podamos obtener una página web, mediante su URL, y poder analizar su contenido HTML con tal de extraer datos e información específica. Nuestro programa ha de ser capaz de cumplir con los siguientes pasos: f**

**\*\*1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.**

**\*\*El primer paso para realizar tareas de crawling y scraping es poder descargar los datos de la web. Para esto usaremos la capacidad de R y de sus librerías (httr y XML) para descargar webs y almacenarlas en variables que podamos convertir en un formato fácil de analizar (p.e. de HTML a XML)..**

Cargamos las librerias de XML i HTTR para posteriormente cargar los datos parseados en la variable parsedhtml.

```{r 1, echo=FALSE}
library(XML)
library(httr)
html <- GET("https://www.mediawiki.org")
content <- content(html, as = "text")
parsedHtml <- htmlParse(content, asText = TRUE)

```

2.  **Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”). En las cabeceras web encontramos información como el título, los ficheros de estilo visual, y meta-información como el nombre del autor de la página, una descripción de esta, el tipo de codificación de esta, o palabras clave que indican qué tipo de información contiene la página. Una vez descargada la página, y convertida a un formato analizable (como XML), buscaremos los elementos de tipo “title”. P.e. “**

    Buscamos en la variable anterior (parsedHtml) donde se encuentra la etiqueta de title y nos guardamos el resultado en la variable title. Mostramos el resultado

```{r 2, echo=FALSE}
title <- xpathSApply(parsedHtml, "//title", xmlValue)
title
```

3.  **Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL. Vamos a extraer, usando las funciones de búsqueda XML, todos los enlaces que salen de esta página con tal de listarlos y poder descargarlas más tarde. Sabemos que estos son elementos de tipo “**<a>**”, que tienen el atributo “href” para indicar la URL del enlace. P.e. “**<a href = ‘enlace’>**Texto del Enlace**</a>**”. Del enlace nos quedaremos con la URL de destino y con el valor del enlace (texto del enlace).**

    Guardamos los 2 resultados en 2 variables (una la del enlace y la otra la del texto.)

```{r 3, echo=FALSE}
links_text <- xpathSApply(parsedHtml, "//a", xmlValue)
links_url <- xpathSApply(parsedHtml, "//a", xmlGetAttr, 'href')

```

```         
**4.Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo. En este paso nos interesa reunir los datos obtenidos en el anterior paso. Tendremos que comprobar, para cada enlace, cuantas veces aparece.**
```

```{r 4, echo=FALSE}
#Convertir variables en df. Eliminamos enlaces vacíos. Recuento de apariciones de cada par único de texto y URL
library(dplyr)
links_df <- data.frame(texto = links_text, url = links_url, stringsAsFactors = FALSE)
links_df <- na.omit(links_df)
links_df <- subset(links_df, texto != "" & url != "")
links_counts <- links_df %>%
group_by(texto, url) %>%
summarise(count = n(), .groups = 'drop')



```

5.  **Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL). En este paso podemos usar la función HEAD de la librería “httr”, que en vez de descargarse la página como haría GET, solo consultamos los atributos de la página o fichero destino. HEAD nos retorna una lista de atributos, y de entre estos hay uno llamado “header” que contiene más atributos sobre la página buscada. Si seguimos podemos encontrar el “status_code” en “resultado\$status_code”. El “status_code” nos indica el resultado de la petición de página o fichero. Este código puede indicar que la petición ha sido correcta (200), que no se ha encontrado (404), que el acceso está restringido (403), etc. 4 Actividad Evaluable 2 Data Driven Securiyt – CyberSecurity Management - 2024 • Tened en cuenta que hay enlaces con la URL relativa, con forma “/xxxxxx/xxxxx/a.html”. En este caso, podemos indicarle como “handle” el dominio de la página que estamos tratando, o añadirle el dominio a la URL con la función “paste”. • Tened en cuenta que puede haber enlaces externos con la URL absoluta, con forma “<http://xxxxxx/xxxx/a.html>” (o https), que los trataremos directamente. • Tened en cuenta que puede haber enlaces que apunten a subdominios distintos, con forma “//subdominio/xxxx/xxxx/a.html”. En este caso podemos adjuntarle el prefijo “https:” delante, convirtiendo la URL en absoluta. • Tened en cuenta URLS internas con tags, como por ejemplo “#search-p”. Estos apuntan a la misma página en la que estamos, pero diferente altura de página. Equivale a acceder a la URL relativa de la misma página en la que estamos. Es recomendado poner un tiempo de espera entre petición y petición de pocos segundos (comando “Sys.sleep”), para evitar ser “baneados” por el servidor. Para poder examinar las URLs podemos usar expresiones regulares, funciones como “grep”, o mirar si en los primeros caracteres de la URL encontramos “//” o “http”. Para tratar las URLs podemos usar la ayuda de la función “paste”, para manipular cadenas de caracteres y poder añadir prefijos a las URLs si fuera necesario.**

```{r 5, echo=FALSE}
library(httr)
library(dplyr)
#Instalación de librerías. Creamos un df para guardar los resultados. Verificamos si la URL es absoluta o relativa. Repetimos sobre los enlaces y obtención de el código de estado. Utilizamos Sys.sleep con 1s para no sobrecargar la URL.
results <- data.frame(
Enlace = links_url,
Texto = links_text,
Visto = integer(length(links_url)),
Estado = integer(length(links_url)),
stringsAsFactors = FALSE
)
make_absolute_url <- function(url, domain = "https://www.mediawiki.org") {
  if (startsWith(url, "http")) { #si comienza con http es relativa
    return(url)
  } else {
    return(paste0(domain, url))
  }
}
for (i in seq_along(results$Enlace)) {
absolute_url <- make_absolute_url(results$Enlace[i])
response <- HEAD(absolute_url)
results$Estado[i] <- status_code(response)
Sys.sleep(1)
}

results <- results %>%
group_by(Enlace, Texto) %>%
summarise(
Visto = n(), #Cuenta el número de filas en cada grupo
Estado = first(Estado),
.groups = 'drop' #Indicamos a dplyr que no queremos que el resultado esté agrupado.
  )




```

**1. Un histograma con la frecuencia de aparición de los enlaces, pero separado por URLs absolutas (con “http…”) y URLs relativas.**

```{r 47, echo=FALSE}
library(dplyr)
library(ggplot2)
library(stringr)
links_df <- links_df %>%
mutate(tipo_enlace = if_else(str_starts(url, "http"), "Absoluto", "Relativo"))
hist(links_counts$count)

```

**2. Un gráfico de barras indicando la suma de enlaces que apuntan a otros dominios o servicios (distinto a <https://www.mediawiki.org> en el caso de ejemplo) vs. la suma de los otros enlaces. Aquí queremos distinguir enlaces que apuntan a mediawiki versus el resto. Sabemos que las URLs relativas ya apuntan dentro, por lo tanto hay que analizar las URLs absolutas y comprobar que apunten a <https://www.mediawiki.org>.**

```{r 23, echo=FALSE}
library(ggplot2)
es_url_interna <- function(url) {
  return(grepl("https://www.mediawiki.org", url))
}
links_df$tipo_url <- ifelse(es_url_interna(links_df$url), "interna", "externa")
conteo_urls <- table(links_df$tipo_url)
links_df_sin_na <- na.omit(links_df)
conteo_urls_sin_na <- table(links_df_sin_na$tipo_url)
ggplot(data = as.data.frame(conteo_urls_sin_na), aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity") +
  labs(x = "Tipo de URL", y = "Cantidad de enlaces", title = "Comparación de enlaces internos vs externos")

```

**3. Un gráfico de tarta (pie chart) indicando los porcentajes de Status de nuestro análisis. Por ejemplo, si hay 6 enlaces con status “200” y 4 enlaces con status “404”, la tarta mostrará un 60% con la etiqueta “200” y un 40% con la etiqueta “404”. Este gráfico lo uniremos a los anteriores. El objetivo final es obtener una imagen que recopile los gráficos generados**

```{r 63, echo=FALSE}
library(ggplot2)
status_counts <- table(results$Estado) #Calculamos frecuencia de cada estado
status_df <- as.data.frame(status_counts)#Convierte la tabla en un df
names(status_df) <- c("Estado", "Frecuencia") #nombre a las columnas
status_df$Porcentaje <- status_df$Frecuencia / sum(status_df$Frecuencia) * 100 #calculo de porcentajes
pie_chart <- ggplot(status_df, aes(x = "", y = Porcentaje, fill = Estado)) +
  geom_bar(width = 1, stat = "identity") + coord_polar("y") +  theme_void() +  geom_text(aes(label = paste0(round(Porcentaje, 1), "%")), position = position_stack(vjust = 0.5)) + scale_fill_discrete(name = "Estado HTTP") +   labs(title = "Distribución de porcentajes de los estados HTTP") #Creación y ajustes de la pie chart
print(pie_chart)

```

**2. Análisis de logs de servidor usando R (parte II)**

**1. Descomprimir el fichero comprimido que contiene los registros del servidor, y a partir de los datos extraídos, cargar en data frame los registros con las peticiones servidas.**

```{r 667, echo=FALSE}
#Descomprimimos el archivo y los leemos en un DF
unzip("epa-http.zip")
datos4 <- read.csv('epa-http.csv',sep = " ",header = FALSE)
```

2.  **Incluid en el documento un apartado con la descripción de los datos analizados: fuente, tipología, descripción de la información contenida (los diferentes campos) y sus valores.**

```{r 34, echo=FALSE}
str(datos4)
summary(datos4)
unique(datos4$campo_categorico)

cat("La fuente de datos es un archivo CSV con registros de logs de un servidor web. Contiene información sobre las solicitudes HTTP realizadas al servidor.\n")

cat("La tipología de los datos es la siguiente:\n")
cat("- V1: Dirección IP del cliente que realizó la solicitud.\n")
cat("- V2: Fecha y hora de la solicitud.\n")
cat("- V3: Tipo de solicitud HTTP y la URL solicitada.\n")
cat("- V4: Código de respuesta HTTP.\n")
cat("- V5: Tamaño de la respuesta en bytes.\n")

cat("Descripción de la información contenida en cada columna:\n")
str(datos4)
cat("\nValores únicos para el campo categórico V4:\n")
unique(datos4$V4)

cat("Resumen estadístico de las columnas numéricas:\n")
summary(datos4)


```

**Limpieza de los Datos**

**3. Aprovechando que los datos a analizar son los mismos de la primera práctica, para esta entrega es imprescindible que los datos estén en formato de “datos elegantes”.**

```{r 554, echo=FALSE}
library(dplyr)
library(tidyr)
library(stringr)
ips_get_separate<-tidyr::separate(datos4,timestamp,c("d","h","s","ms"),sep=":")
#ips_get_separate$d <- gsub("[", "", ips_get_separate$d)
#ips_get_separate$ms <- gsub("]", "", ips_get_separate$ms)
ips_get_separate$d <- substr(ips_get_separate$d, 2, nchar(ips_get_separate$d))
ips_get_separate$ms <- substr(ips_get_separate$ms, 1, nchar(ips_get_separate$ms) - 1)

subcadenes<-str_split(datos4$V3," ", simplify = TRUE)
colnames(datos4)<-c("IPs","Timestamp","Petición","Respuesta","Bytes")
ips_get_separate<-tidyr::separate(ips_get_separate,V3,c("Tipo","URL","Protocolo"),sep=" ")
colnames(ips_get_separate)<-c("IPs","d","h","s","ms","Tipo","URL","Protocolo","Respuesta","Bytes")
datos_limpios<-ips_get_separate

```

**\*\*Exploración de datos**

4.  **Identificar el número único de usuarios que han interactuado directamente con el servidor de forma segregada según si los usuarios han tenido algún tipo de error en las distintas peticiones ofrecidas por el servidor.**

```{r 9999, echo=FALSE}
#Buscamos todos los que en el campo de respuesta empieza por 3 porque son los que salen con errores. Una vez tenemos el número, lo restamos al total para saber los que han funcionado ok
datos_limpios2 <- subset(datos_limpios, grepl("^3", datos_limpios$Respuesta))
nrespuestas_error<-length(grep("^3",datos_limpios$Respuesta))
nrespuestas_ok<-nrow(datos_limpios)-nrespuestas_error
nrespuestas_error
nrespuestas_ok

```

**Análisis de Datos**

5.  **Analizar los distintos tipos de peticiones HTTP (GET, POST, PUT, DELETE) gestionadas por el servidor, identificando la frecuencia de cada una de estas. Repetir el análisis, esta vez filtrando previamente aquellas peticiones correspondientes a recursos ofrecidos de tipo imagen.**

```{r 941, echo=FALSE}
library(dplyr)
datos_limpios_imagen<-subset(datos_limpios, grepl("\\.gif$", datos_limpios$URL))
datos_agrupados_imagen <- datos_limpios_imagen %>% group_by(Tipo) %>% summarise(total = sum(as.integer(Bytes)), n = n())
```

**Visualización de Resultados**

6.  **Generar al menos 2 gráficos distintos que permitan visualizar alguna característica relevante de los datos analizados. Estos deberán representar por lo menos 1 o 2 variables diferentes del data frame. Describid el gráfico e indicad cualquier observación destacable que se pueda apreciar gracias a la representación gráfica.**

```{r 948, echo=FALSE}


```

7.  **Generar un gráfico que permita visualizar el número de peticiones servidas a lo largo del tiempo.**

```{r 923, echo=FALSE}


```

8.  **Utilizando un algoritmo de aprendizaje no supervisado, realizad un análisis de clústering con k-means para los datos del servidor. • Para este análisis debéis repetir la ejecución del modelado con distintos valores de k (número de clústeres) con al menos 2 valores diferentes dek.\
    • A fin de retener algo de información sobre el recurso servido, generad una columna numérica derivada de esta con el número de caracteres de la URL servida**

```{r 903, echo=FALSE}
library(mltools)
library(data.table)
library(cluster)

#eppa_http_one_hot <- one_hot(as.data.table(datos_agrupados_imagen), sparsifyNAs= TRUE)

# Crear datos de ejemplo
set.seed(123)
datos <- matrix(as.integer(results$Estado), ncol = 2)

# Especificar el número de clusters (k)
k <- 3
k2 <- 1

# Aplicar el algoritmo k-means
resultados_kmeans <- kmeans(datos, centers = k)
resultados_kmeans2 <- kmeans(datos, centers = k2)
# Mostrar los resultados
print(resultados_kmeans)


```

9.  **Representad visualmente en gráficos de tipo scatter plot el resultado de vuestros clústering y interpretad el resultado obtenido (describid las características de los distintos grupos) con los 2 valores distintos de k probados en el apartado anterior en función de los valores de las variables y el número de clúster asignado.**

```{r 9103, echo=FALSE}
# Crear un gráfico de dispersión con colores según los clusters
plot(datos, col = resultados_kmeans$cluster, pch = 19, main = "k-means Clustering", xlab = "Variable 1", ylab = "Variable 2")

# Agregar los centroides al gráfico
points(resultados_kmeans$centers, col = 1:k, pch = 8, cex = 2)

# Crear otro gráfico de dispersión con colores según los clusters
plot(datos, col = resultados_kmeans2$cluster, pch = 19, main = "k-means Clustering 2", xlab = "Variable 1", ylab = "Variable 2")

# Agregar los centroides al gráfico
points(resultados_kmeans2$centers, col = 1:k, pch = 5, cex = 2)

# En la comaparcion de los 2 gráficos podemos ver que si solo hay un centro (este se poner en la parte superior izquierda) y si hay 3, se reparte por distintos sitios del gráfico.


```
