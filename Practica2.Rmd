---
title: "Parte 2"
author: "Pablo"
date: "2024-01-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
summary(cars)
```

**Pregunta 1:**

**\*\*Queremos programar un programa de tipo web scrapping con el que podamos obtener una página web, mediante su URL, y poder analizar su contenido HTML con tal de extraer datos e información específica. Nuestro programa ha de ser capaz de cumplir con los siguientes pasos: f**

**\*\*1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.**

**\*\*El primer paso para realizar tareas de crawling y scraping es poder descargar los datos de la web. Para esto usaremos la capacidad de R y de sus librerías (httr y XML) para descargar webs y almacenarlas en variables que podamos convertir en un formato fácil de analizar (p.e. de HTML a XML)..**

Cargamos las librerias de XML i HTTR para posteriormente cargar los datos parseados en la variable parsedhtml.

```{r 1, echo=FALSE}
library(XML)
library(httr)
html <- GET("https://www.mediawiki.org")
content <- content(html, as = "text")
parsedHtml <- htmlParse(content, asText = TRUE)

```

2.  **Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”). En las cabeceras web encontramos información como el título, los ficheros de estilo visual, y meta-información como el nombre del autor de la página, una descripción de esta, el tipo de codificación de esta, o palabras clave que indican qué tipo de información contiene la página. Una vez descargada la página, y convertida a un formato analizable (como XML), buscaremos los elementos de tipo “title”. P.e. “**

    Buscamos en la variable anterior (parsedHtml) donde se encuentra la etiqueta de title y nos guardamos el resultado en la variable title. Mostramos el resultado

```{r 2, echo=FALSE}
title <- xpathSApply(parsedHtml, "//title", xmlValue)
title
```

3.  **Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL. Vamos a extraer, usando las funciones de búsqueda XML, todos los enlaces que salen de esta página con tal de listarlos y poder descargarlas más tarde. Sabemos que estos son elementos de tipo “**<a>**”, que tienen el atributo “href” para indicar la URL del enlace. P.e. “**<a href = ‘enlace’>**Texto del Enlace**</a>**”. Del enlace nos quedaremos con la URL de destino y con el valor del enlace (texto del enlace).**

    Guardamos los 2 resultados en 2 variables (una la del enlace y la otra la del texto.)

```{r 3, echo=FALSE}
links_text <- xpathSApply(parsedHtml, "//a", xmlValue)
links_url <- xpathSApply(parsedHtml, "//a", xmlGetAttr, 'href')

```

```         
**4.Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo. En este paso nos interesa reunir los datos obtenidos en el anterior paso. Tendremos que comprobar, para cada enlace, cuantas veces aparece.**
```

```{r 4, echo=FALSE}
#Convertir variables en df. Eliminamos enlaces vacíos. Recuento de apariciones de cada par único de texto y URL
library(dplyr)
links_df <- data.frame(texto = links_text, url = links_url, stringsAsFactors = FALSE)
links_df <- na.omit(links_df)
links_df <- subset(links_df, texto != "" & url != "")
links_counts <- links_df %>%
group_by(texto, url) %>%
summarise(count = n(), .groups = 'drop')



```

5.  **Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL). En este paso podemos usar la función HEAD de la librería “httr”, que en vez de descargarse la página como haría GET, solo consultamos los atributos de la página o fichero destino. HEAD nos retorna una lista de atributos, y de entre estos hay uno llamado “header” que contiene más atributos sobre la página buscada. Si seguimos podemos encontrar el “status_code” en “resultado\$status_code”. El “status_code” nos indica el resultado de la petición de página o fichero. Este código puede indicar que la petición ha sido correcta (200), que no se ha encontrado (404), que el acceso está restringido (403), etc. 4 Actividad Evaluable 2 Data Driven Securiyt – CyberSecurity Management - 2024 • Tened en cuenta que hay enlaces con la URL relativa, con forma “/xxxxxx/xxxxx/a.html”. En este caso, podemos indicarle como “handle” el dominio de la página que estamos tratando, o añadirle el dominio a la URL con la función “paste”. • Tened en cuenta que puede haber enlaces externos con la URL absoluta, con forma “<http://xxxxxx/xxxx/a.html>” (o https), que los trataremos directamente. • Tened en cuenta que puede haber enlaces que apunten a subdominios distintos, con forma “//subdominio/xxxx/xxxx/a.html”. En este caso podemos adjuntarle el prefijo “https:” delante, convirtiendo la URL en absoluta. • Tened en cuenta URLS internas con tags, como por ejemplo “#search-p”. Estos apuntan a la misma página en la que estamos, pero diferente altura de página. Equivale a acceder a la URL relativa de la misma página en la que estamos. Es recomendado poner un tiempo de espera entre petición y petición de pocos segundos (comando “Sys.sleep”), para evitar ser “baneados” por el servidor. Para poder examinar las URLs podemos usar expresiones regulares, funciones como “grep”, o mirar si en los primeros caracteres de la URL encontramos “//” o “http”. Para tratar las URLs podemos usar la ayuda de la función “paste”, para manipular cadenas de caracteres y poder añadir prefijos a las URLs si fuera necesario.**

```{r 5, echo=FALSE}


```
